
\chapter{Descripción de los servicios de detección facial \label{chap:Ambiente-de-desarrollo}}


\section*{Servicio SkyBiometry}


\subsection*{En referencia al uso de la API – Autenticación }

Cada llamada a la API debe ser autorizada mediante el uso de dos claves:
api\_key y api\_secret. Ambas claves son obtenidas mediante una registración
en el sitio oficial de skybiometry (\emph{www.skybiometry.com}) a
través de una cuenta de email como usuario y una contraseña.


\subsection*{Límites de uso }

El consumo de este servicio presenta algunas limitaciones que varían
en base a la suscripción particular del usuario. En general, las suscripciones
gratuitas restringen un límite de 100 llamadas a métodos por hora
y de 5000 en el mes. Para el caso del método POST, la limitación se
presenta tanto en 100 request a la API para el procesamiento de imágenes
individuales como para 1 request con 100 imágenes a procesar. 


\subsection*{Factores ofrecidos}
\begin{lyxlist}{00.00.0000}
\item [{Respecto~al~cuadrado~del~rostro~detectado:}]~

\begin{lyxlist}{00.00.0000}
\item [{center~OBJECT~\{~x~:Float,~y~:~Float~\}}]~
\item [{width.~FLOAT.~0-100\%~del~ancho~del~rostro~respecto~al~ancho~de~la~imagen.}]~
\item [{height.~FLOAT.~0-100\%~del~largo~del~rostro~respecto~al~ancho~de~la~imagen.}]~
\end{lyxlist}
\item [{Otros~puntos~detectados:}]~

\begin{lyxlist}{00.00.0000}
\item [{mouth\_center~OBJECT~\{x:Float,~y:~Float~\}}]~
\item [{mouth\_left~OBJECT~\{x:~Float,~y:~Float~\}}]~
\item [{mouth\_right~OBJECT~\{x:~Float,~y:~Float~\}}]~
\item [{eye\_left~OBJECT~\{x:~Float,~y:~Float~\}}]~
\item [{eye\_right~OBJECT~\{x:~Float,~y:~Float~\}}]~
\item [{nose~OBJECT~\{x:~Float,~y:~Float~\}}]~
\item [{ear\_left~OBJECT~\{x:~Float,~y:~Float~\}}]~
\item [{ear\_right~OBJECT~\{x:~Float,~y:~Float~\}}]~
\item [{chin~OBJECT~\{x:~Float,~y:~Float~\}}]~
\item [{yaw~FLOAT.Perfil.~Value~-90°~a~90°}]~
\item [{roll~FLOAT.~Ángulo~de~rotación~del~rostro.~Value~-90°~a~90°}]~
\item [{pitch~FLOAT.~Value~-90°~a~90°}]~
\end{lyxlist}
\end{lyxlist}
\begin{description}
\item [{yaw:}] ángulo positivo en rostros donde predomina el perfil derecho
(respecto al sujeto, no a la imagen). Caso contrario, ángulo negativo
al predominar el perfil izquierdo. 
\item [{Roll:}] Ejemplo ilustrativo. El rectángulo verde representa al
rostro detectado por la API cuyo valor roll es igual a -17°, mientras
que el rectángulo rojo fue añadido para ilustrar el caso donde el
ángulo roll es igual a 0°. Una rotación inversa significa un valor
roll positivo. 
\item [{\includegraphics{\string"C:/Users/usuario/Desktop/Tesis rubbish/tesis/images/face-example\string".png}}]~
\end{description}
Todos los puntos detectados descritos no especificados corresponden
a un formato JSON, cuya clave es el nombre del mismo, y el valor es
también un objeto del tipo JSON que contiene los siguientes valores:
id:INTEGER, confidence:INTEGER, y:FLOAT, x:FLOAT, a excepción del
valor \emph{center} cuyo objeto JSON contiene sólo los valores: \emph{x},
\emph{y}.
\begin{description}
\item [{Nota1:}] en caso de valor de atributo no se puede determinar que
no se devuelve. Si se determina el valor que se devuelve junto con
el valor de confianza como porcentaje de 0 a 100. 
\item [{Nota2:}] La API sólo devuelve información necesaria para representar
en forma de rectángulo los rostros detectados. Para el resto de opciones,
la API sólo devuelve información del punto central en cuestión. 
\item [{Nota3:}] Ya que la API automáticamente re-escala las imágenes a
1024 pixeles para su procesamiento interno, todas las coordenadas
son provistas de forma porcentual respecto al ancho y largo de la
imagen (abstracción). 
\item [{Nota4:}] Nota: el atributo face es el valor por defecto y siempre
es retornado, independientemente de los atributos especificados. Si
el atributo \emph{glasses} fue requerido adicionalmente se retorna
el atributo \emph{dark\_glasses} para diferenciar entre gafas oscuras
y claras. El atributo mood (estado de ánimo) se devuelve junto con
7 más atributos: confianza para cada una de las emociones básicas,
además de la confianza neutral\_mood. 
\end{description}

\subsection*{Atributos faciales }

The result of the API call is a JSON or XML object containing the
requested information. Cada uno de los atributos son objetos del tipo
JSON cuya clave es el nombre del mismo y el valor es también un objeto
JSON con dos valores específicos: el primero con clave \emph{value}
y el segundo con clave \emph{confidence}. El valor de confidence es
del tipo Integer para representar el porcentaje de probabilidad del
valor detectado en el campo \emph{value}.


\subsection*{Mensajes de error }

\includegraphics[scale=0.55]{\string"C:/Users/usuario/Desktop/Tesis rubbish/tesis/images/face-service-errors\string".png}


\subsection*{Post (Request en lenguaje JAVA)}

\begin{lstlisting}[language=Java,numbers=left,numberstyle={\tiny},basicstyle={\footnotesize},breaklines=true,captionpos=t,frame=no,keywordstyle={\color{blue}},commentstyle={\color{gray}},stringstyle={\color{red}},numbersep=5pt,emph={label}]
HttpResponse<JsonNode> response = Unirest.post(https://face.p.mashape.com/faces/detect?api_key=[api_key]&api_secret=[api_secret]) 
	.header(X-Mashape-Key, 7rS5YDw5YHmshtdgMHP2ZYBLAljfp1OxKNzjsn1GJxNBgad6C9) 
	.header(Accept, application/json) 
	.field(attributes, all) 
	.field(detector, Aggressive)  //[ver opciones] 
	.asJson();        
\end{lstlisting}

\begin{description}
\item [{Nota:}] En los espacios {[}api\_key{]} y {[}api\_secret{]} deben
colocarse las claves correspondientes disponibles con la registración
de una cuenta en el sitio www.skybiometry.com \end{description}
\begin{lyxlist}{00.00.0000}
\item [{Opciones:}]~

\begin{lyxlist}{00.00.0000}
\item [{.field(files,~Vector<File>~imagenes)}]~
\item [{.field(files,~File~imagen)}]~
\item [{.field(urls,~url\_1,~url\_2,~…)}]~
\end{lyxlist}
\end{lyxlist}
\begin{description}
\item [{Nota:}] Los formatos de imagen aceptados por la API son los siguientes:
PNG, JPEG, BMP, JPEG2000.
\end{description}
La respuesta recibida por el servicio del tipo HttpResponse<JsonNode>
puede ser manipulada como un objeto del tipo JSONObject a través de
la siguiente codificación: 

\begin{lstlisting}[language=Java,breaklines=true]
JSONObject obj = response.getBody().getObject(); 
\end{lstlisting}



\subsection*{Parse - Overview }

La respuesta en formato JSONObject contiene en primera parte un objeto
del tipo JSONObject con clave \emph{photos} y cuyo valor, representado
en formato JSONArray, contiene toda la información detectada desde
las imágenes. Cada elemento del arreglo representa un archivo de imagen;
a su vez, estos elementos contienen un objeto del tipo JSONObject
cuya clave es \emph{tags} y su valor es un JSONArray para representar
cada una de los rostros detectados en la imagen particular; a su vez
contiene la información de ancho y largo asociado. 

En segunda parte, seguido de \emph{photos} se anexan en la respuesta
la información relacionada a la operación http.

Ejemplo codificación básica: 

\begin{lstlisting}[language=Java,numbers=left,numberstyle={\tiny},basicstyle={\footnotesize},breaklines=true,captionpos=t,frame=no,keywordstyle={\color{blue}},commentstyle={\color{gray}},stringstyle={\color{red}},numbersep=5pt,emph={label}]
JSONArray imágenes = obj.get(photos); 
for(int p=0; p<imágenes.length(); p++) { 
	//Obtención de cada una de las imágenes  	
	JSONObject imagen=imagenes.getJSONObject(p);   
	//Ejemplo obtención del ancho de la imagen 
	int imgWidth = (int) imagen.get(width); 
	//Obtención de los rostros detectados en la imagen 
	JSONArray rostros = imagen.getJSONArray(tags); 
	for(int r=0; r<rostros.lenght(); r++) { 
		//Obtención de cada uno de los rostros detectados 
		rostro=rostros.getJSONObject(r);     	 
		//Ejemplo obtención de los puntos detectados JSONObject 
		mouth = rostro.getJSONObject(mouth_center); 
		int coordX= (int) mouth.get(“x”); 
		int coordY= (int) mouth.get(“y”); 
		//Ejemplo obtención de los atributos faciales JSONObject 
		atributos=imagen.getJSONObject(“attributes”); 
		String valor = atributos.getJSONObject(“mood”).getString(“value”); 
	} 	
} 
\end{lstlisting}



\section*{Servicio FaceRect}


\subsection*{En referencia al uso de la API }

El consumo de esta API requiere la registración de una cuenta en el
sitio Mashape. La registración se realiza con email y contraseña y
se realiza de manera gratuita e instantánea. El servicio no restringe
el número de request a la API pero permite el procesamiento de sólo
una imagen por request realizado. 


\subsection*{Factores ofrecidos }
\begin{lyxlist}{00.00.0000}
\item [{Rostro~detectado:}]~

\begin{lyxlist}{00.00.0000}
\item [{Orientation:~tipo~String.~Valores~posibles:<frontal,~profile-right~profile-left>}]~
\item [{x:~tipo~Integer}]~
\item [{y:~tipo~Integer}]~
\item [{width:~tipo~Integer}]~
\item [{height:~tipo~Integer}]~
\item [{Features:~(opcional)}]~
\item [{eyes}]~
\item [{nose}]~
\item [{mouth}]~
\end{lyxlist}
\end{lyxlist}
Los \emph{features} son analizados por la API sólo en rostros cuya
orientación sea frontal. El formato de respuesta de los mismos son
del tipo JSONObject en el caso de \emph{nose} y \emph{mouth}. Para
el caso de \emph{eyes} se devuelve un objeto del tipo JSONArray, cuyos
elementos (1 o 2) son del tipo JSONObject. La API representa el rostro
y los features detectados en forma de rectángulos, por lo que incorpora
en sus respuestas las coordenadas \emph{x}, \emph{y} (esquina superior
izquierda respecto a la imagen) y los tamaños \emph{width}, \emph{height}
del rectángulo en cuestión.


\subsection*{Post (Request en lenguaje JAVA)}

\lstinline[language=Java,numbers=left,numberstyle={\tiny},basicstyle={\footnotesize},breaklines=true,captionpos=t,frame=no,keywordstyle={\color{blue}},commentstyle={\color{gray}},stringstyle={\color{red}},numbersep=5pt,emph={label}]!HttpResponse<JsonNode> response = Unirest.post(ENDPOINT) 	.header(X-Mashape-Key, 7rS5YDw5YHmshtdgMHP2ZYBLAljfp1OxKNzjsn1GJxNBgad6C9) //[ver opción correspondiente] 	.field(“features”, true) 	.asJson();        !

Actualmente la API soporta dos endpoints difiriendo entre sí en el
modo en que las imágenes son especificadas en el request. Mediante
el método http \emph{GET} se añade el URL de la imagen (Endpoint:
http://apicloud-facerect.p.mashape.com/process-url.json); por otra
parte con el método \emph{POST} se añade el archivo correspondiente
(Endpoint: http://apicloud-facerect.p.mashape.com/process-file.json)
De acuerdo con el endpoint especificado se añade la línea correspondiente: 
\begin{lyxlist}{00.00.0000}
\item [{Opciones}]~

\begin{lyxlist}{00.00.0000}
\item [{.field(url,~url\_image)}]~
\item [{.field(image,~new~File(<path\_image>))}]~
\end{lyxlist}
\end{lyxlist}
\begin{description}
\item [{Nota:}] Para ambos casos, la API restringe el procesamiento de
algunas imágenes, permitiendo procesar aquellas cuyo formato sea JPEG,
PNG y GIF, la resolución de la misma no supere los 4096 píxeles (4096×4096)
y el tamaño sea inferior a 10 MBytes. 
\end{description}
La respuesta recibida por el servicio del tipo HttpResponse<JsonNode>
puede ser manipulada como un objeto del tipo JSONObject a través de
la siguiente codificación: 

\begin{lstlisting}[language=Java,breaklines=true]
JSONObject obj = response.getBody().getObject();
\end{lstlisting}



\subsection*{Parse - overview}

La respuesta en formato JSONObject contiene dos objetos JSON. El primero,
cuya clave es \emph{faces} contiene toda la información detectada
desde la imagen, el segundo objeto cuya clave es \emph{image} contiene
la información acerca del ancho (\emph{width}) y largo (\emph{height})
de la misma. La información detectada se presenta en un objeto del
tipo JSONArray donde cada elemento del arreglo constituye cada uno
de los rostros que ha detectada la API; Cada uno de los elementos
son JSONObject que contienen la información del rostro (\emph{orientation},
\emph{x}, \emph{y}, \emph{height}, \emph{width}) y los \emph{features}. 

Ejemplo codificación básica 

\begin{lstlisting}[language=Java,numbers=left,numberstyle={\tiny},basicstyle={\footnotesize},breaklines=true,captionpos=t,frame=no,keywordstyle={\color{blue}},commentstyle={\color{gray}},stringstyle={\color{red}},numbersep=5pt,emph={label}]
//Ejemplo obtención del ancho de la imagen 
int imgWidth = (int) obj.getJSONObject(image).get(“width”); 
//Obtención de los rostros detectados en la imagen 
JSONArray rostros = obj.getJSONArray(faces); 
for(int r=0; r<rostros.lenght(); r++) {  	
	//Obtención de cada uno de los rostros detectados 
	rostro=rostros.getJSONObject(r);     	 
	//Ejemplo obtención información del rostro: 
	String orientation =(String) rostro.get(“orientation); 
	int coordX=(int) rostro.get(“x”); 
	//Ejemplo obtención de los features 
	JSONObject features = rostro.getJSONObject(“features”); 
	JSONArray eyes =features.getJSONArray(“eyes”); 
	JSONObject eye_left=eyes.getJSONObject(0); 
}
\end{lstlisting}



\section*{Servicio GMS Vision}


\subsection*{Factores ofrecidos}

El detector puede computar los siguientes atributos (accesibles a
través de los distintos métodos de la clase).

Profile: rotación del rostro. FLOAT 
\begin{lyxlist}{00.00.0000}
\item [{Clasificadores:<característica~facial~presente~en~el~rostro>}]~

\begin{lyxlist}{00.00.0000}
\item [{Open~eyes:~(probabilidad)~Constant~value:~1~FLOAT~({*})}]~
\item [{Smiling:~(probabilidad)~Constant~value:~1~FLOAT~({*})}]~
\item [{Height,~width:~medidos~en~píxeles.~FLOAT}]~
\item [{Position:~Punto~superior~izquierdo~del~rostro.~FLOAT}]~
\end{lyxlist}
\item [{Landmarks:<Puntos~de~interés~en~el~rostro~detectado>~LIST<Landmark>}]~

\begin{lyxlist}{00.00.0000}
\item [{BOTTOM\_MOUTH~Constant~Value:~0~CENTER}]~
\item [{LEFT\_CHEEK~Constant~Value:~1~CENTER}]~
\item [{LEFT\_EAR\_TIP~Constant~Value:~2}]~
\item [{LEFT\_EAR~Constant~Value:~3}]~
\item [{MIDPOINT~LEFT\_EYE~Constant~Value:~4}]~
\item [{CENTER~LEFT\_MOUTH~Constant~Value:~5}]~
\item [{NOSE\_BASE~Constant~Value:~6}]~
\item [{MIDPOINT~RIGHT\_CHEEK~Constant~Value:~7}]~
\item [{CENTER~RIGHT\_EAR\_TIP~Constant~Value:~8}]~
\item [{RIGHT\_EAR~Constant~Value:~9}]~
\item [{MIDPOINT~RIGHT\_EYE~Constant~Value:~10}]~
\item [{CENTER~RIGHT\_MOUTH~Constant~Value:~11}]~
\end{lyxlist}
\item [{Angles:~<Face~Orientation>}]~

\begin{lyxlist}{00.00.0000}
\item [{EulerY~{[}valor~y~en~la~imagen{]}}]~
\item [{EulerZ~{[}valor~r~en~la~imagen{]}}]~
\end{lyxlist}
\item [{({*})}] En el trabajo desarrollado se consideró el uso de valores
booleanos, determinando a los clasificadores como verdaderos para
valores de probabilidad mayor al 50\%, en casos contrario se determinaron
los mismos como falsos. \end{lyxlist}
\begin{description}
\item [{Nota:}] El ángulo EulerZ se encuentra disponible siempre en la
detección, mientras que el ángulo EulerY sólo si en el detector se
predetermina el modo \emph{accurate}. El concepto de \emph{left} y
\emph{right} son relativos al sujeto no a la posición cuando se observa
la imagen. Las posiciones son accedidas a través del método getPosition()
de la clase Landmark; el retorno es del tipo PointF. 
\end{description}

\subsection*{Construcción del detector }

\begin{lstlisting}[language=Java,numbers=left,numberstyle={\tiny},basicstyle={\footnotesize},breaklines=true,captionpos=t,frame=no,keywordstyle={\color{blue}},commentstyle={\color{gray}},stringstyle={\color{red}},numbersep=5pt,emph={label}]
FaceDetector detector = new FaceDetector.Builder(context) 
	.setTrackingEnabled(false) 
	.setMode(int Mode) 
	.setProminentFaceOnly(true) 
	.setClassificationType(int classificationType) 
	.setLandmarkType(int landmarkType) 
	.build(); 
\end{lstlisting}


Es necesario especificar en el detector el entorno de nuestra aplicación
(mediante el uso de la clase android.utils.Context) 

Es posible obtener el entorno de la app a través del método \emph{getApplicationContext()}
de la clase Activity. 

El contexto es sólo una interfaz cuya implementación está prevista
por el sistema android y permite el acceso a los recursos y clases
específicas de la aplicación, como así también realizar operaciones
tales como el despacho de actividades, la difusión y recibimiento
de objetos del tipo Intent, etc. 

El servicio dispone de la posibilidad de elegir la preferencia de
procesamiento en la detección, \emph{FAST\_MODE} prioriza la rapidez
en el análisis por sobre la cantidad de rostros detectados y su precisión,
contrariamente el servicio dispone del modo \emph{ACCURATE\_MODE}

Habilitar la opción \emph{tracking enabled} es recomendable para la
detección de imágenes individuales no relacionadas (a diferencia de
un vídeo o una serie de imágenes fijas capturadas consecutivamente). 

Tanto los clasificadores y landmarks pueden especificarse uno a uno
añadiendo las líneas correspondientes en el constructor; adicionalmente
la clase FaceDetector proporciona las siguientes constantes: 
\begin{lyxlist}{00.00.0000}
\item [{\emph{ALL\_CLASSIFICATIONS}}]~
\item [{\emph{ALL\_LANDMARKS}}]~
\item [{\emph{NO\_CLASSIFICATIONS}}]~
\item [{\emph{NO\_LANDMARKS}}]~
\end{lyxlist}
El servicio no ofrece la capacidad de setar los landmark y clasificadores
bajo demanda, sino que son definidos estáticamente al momento de crear
el detector; esta restricción condiciona el tiempo de respuesta del
servicio ya que el requerimiento de al menos un landmark, por ejemplo,
significa el uso de la variable \emph{FaceDetector.ALL\_LANDMARKS},
incrementando así el tiempo de procesamiento de la imagen. 


\subsection*{Uso del detector}

El detector se puede llamar de forma sincronizada con un objeto del
tipo Frame para detectar las caras.

\begin{lstlisting}[language=Java,numbers=left,numberstyle={\tiny},basicstyle={\footnotesize},breaklines=true,captionpos=t,frame=no,keywordstyle={\color{blue}},commentstyle={\color{gray}},stringstyle={\color{red}},numbersep=5pt,emph={label}]
Frame frame = new Frame.Builder().setBitmap(<file_image>).build(); 
SparseArray<Face> faces = detector.detect(frame); 
\end{lstlisting}



\subsection*{Ejemplo codificación básica}

\begin{lstlisting}[language=Java,numbers=left,numberstyle={\tiny},basicstyle={\footnotesize},breaklines=true,captionpos=t,frame=no,keywordstyle={\color{blue}},commentstyle={\color{gray}},stringstyle={\color{red}},numbersep=5pt,emph={label}]
for(int i=0;i<faces.size();i++){ 
	//Obtención de cada uno de los rostros detectados en la imagen 
	Face rostro=faces.valueAt(i); 
	//Obtención del rectángulo del rostro 
	Float coordX=rostro.getPosition().x; 
	Float coordY=rostro.getPosition().y; 
	Float width=rostro.getWidth(); 
	Float height=rostro.getHeight(); 
	//Obtención del ángulo EulerZ 
	Float angleZ=rostro.getEulerZ(); 
	//Obtención del clasificador ‘eyes open’ 
	Float probEyeRight=rostro.getIsRightEyeOpenProbability(); 
	Float probEyeLeft=rostro.getIsLeftEyeOpenProbability(); 
	//Obtención de los Landmarks detectados: 
	List<Landmark> landmarks=rostro.getLandmarks(); 	 
	//Obtención de cada uno de los landmark 
	for(int j=0;j<landmarks.size();j++){ 
		int LandmarkType=landmarks.get(j).getType(); 
		PointF Landmarkposition=landmarks.get(j).getPosition(); 
	} 
} 
\end{lstlisting}

